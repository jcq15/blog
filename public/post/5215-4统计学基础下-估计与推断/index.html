<!DOCTYPE html>
<html lang="zh-hans">

<head>
    
<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
<meta name="HandheldFriendly" content="True" />
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
<meta name="generator" content="Hugo 0.89.2" />



<link rel="apple-touch-icon" sizes="180x180" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/apple-touch-icon.png" />
<link rel="icon" type="image/png" sizes="32x32" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon-32x32.png" />
<link rel="icon" type="image/png" sizes="16x16" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon-16x16.png" />
<link rel="manifest" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/site.webmanifest" />
<link rel="mask-icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/safari-pinned-tab.svg" color="#8aa2d3" />
<link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/amzrk2/cdn-stcapi@1/favicons/favicon.ico" />
<meta name="theme-color" content="#ffffff" />


<title>统计学基础（下）-点估计 - 沙雕园</title>


<meta name="author" content="精神病人" />


<meta name="description" content="A minimal Hugo theme with nice theme color." />


<meta name="keywords" content="学习" />


<meta property="og:title" content="统计学基础（下）-点估计" />
<meta name="twitter:title" content="统计学基础（下）-点估计" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.shadiao.online/post/5215-4%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%E4%B8%8B-%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%8E%A8%E6%96%AD/" /><meta property="og:description" content="妙啊。" />
<meta name="twitter:description" content="妙啊。" /><meta property="og:image" content="https://blog.shadiao.online/img/og.png" />
<meta name="twitter:card" content="summary_large_image" />
<meta name="twitter:image" content="https://blog.shadiao.online/img/og.png" /><meta property="article:published_time" content="2021-12-28T14:13:04+08:00" /><meta property="article:modified_time" content="2021-12-28T14:13:04+08:00" />


<style>
    @media (prefers-color-scheme: dark) {
        body[data-theme='auto'] img {
            filter: brightness(60%);
        }
    }

    body[data-theme='dark'] img {
        filter: brightness(60%);
    }
</style>



<link rel="stylesheet" href="https://blog.shadiao.online/assets/css/fuji.min.css" />





</head>

<body data-theme="auto">
    <script data-cfasync="false">
  
  var fujiThemeData = localStorage.getItem('fuji_data-theme');
  
  if (!fujiThemeData) {
    localStorage.setItem('fuji_data-theme', 'auto');
  } else {
    
    if (fujiThemeData !== 'auto') {
      document.body.setAttribute('data-theme', fujiThemeData === 'dark' ? 'dark' : 'light');
    }
  }
</script>
    <header>
    <div class="container-lg clearfix">
        <div class="col-12 header">
            <a class="title-main" href="https://blog.shadiao.online">沙雕园</a>
            
            <span class="title-sub">博客。</span>
            
        </div>
    </div>
</header>

    <main>
        <div class="container-lg clearfix">
            
            <div class="col-12 col-md-9 float-left content">
                
<article>
    
    <h2 class="post-item post-title">
        <a href="https://blog.shadiao.online/post/5215-4%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%9F%BA%E7%A1%80%E4%B8%8B-%E4%BC%B0%E8%AE%A1%E4%B8%8E%E6%8E%A8%E6%96%AD/">统计学基础（下）-点估计</a>
    </h2>
    <div class="post-item post-meta">
        <span><i class="iconfont icon-today-sharp"></i>&nbsp;2021-12-28</span><span><i class="iconfont icon-file-tray-sharp"></i>&nbsp;4212 字</span><span><i class="iconfont icon-pricetags-sharp"></i>&nbsp;<a href="/tags/%E5%AD%A6%E4%B9%A0">学习</a>&nbsp;</span>

    </div>
    
    <div class="post-content markdown-body">
        <p>妙啊。</p>
<h2 id="点估计">点估计</h2>
<p>设 $X_1,\cdots,X_n\sim P_\theta\in\mathcal{P}$，其中 $\theta=(\theta_1,\cdots,\theta_k)\in\Theta$. 这是经典的参数模型。如前所述，**估计量（estimator）**是统计量 $\hat\theta=w(X_1,\cdots,X_n)$. 我们下面来介绍得到 $\hat\theta$ 的方法。</p>
<h3 id="矩估计">矩估计</h3>
<p>通常来说 $j$ 阶矩都和参数有关，即 $E_\theta X_1^j=h_j(\theta)$. 假设 $\theta$ 是 $k$ 维的，令 $h(\theta)=(h_1(\theta),\cdots,h_k(\theta))$. 若该存在的都存在，就可以淦了：
$$
\hat\theta_j=h_j^{-1}\left(\frac{1}{n}\sum_{i=1}^nX_i^j\right).
$$</p>
<h3 id="极大似然估计">极大似然估计</h3>
<p>似然函数定义为
$$
l(\theta;X)=f_\theta(X).
$$
就是密度函数，不过变量变成了 $\theta$。极大似然估计为
$$
\hat\theta=\mathop{\arg\max}_{\theta\in\Theta}\ l(\theta;X).
$$
对于指数族
$$
l(\eta;x)=\exp\left(\eta^T(\theta)T(x)-\xi(\theta)\right)h(x),
$$
如果各种反函数存在，可以证明 MLE 是
$$
\hat\theta=\eta^{-1}\left(\mu^{-1}(T(x))\right).
$$</p>
<h3 id="m-估计">M-估计</h3>
<p>是 MLE 的推广，似然函数被换成一般函数 $s_\theta:\mathcal{X}\to\bar{\mathbb{R}}$，估计值是使得 $S_n(\theta)=\dfrac{1}{n}\sum_{i=1}^n s_\theta(X_i)$ 最大的 $\theta$.</p>
<h2 id="估计的评价">估计的评价</h2>
<h3 id="一般理论">一般理论</h3>
<p>**决策规则（decision rule）**是将观察结果转换为适当动作的函数。对于一般的统计问题，我们首先获取样本 $X$，记 allowable actions（不知道咋翻译最恰当）为集合 $\mathbb{A}$. 一个决策规则 $T$ 便是把 $X$ 映射为 $\mathbb{A}$ 中 $T(X)$ 的过程。</p>
<p><strong>损失函数（loss function）</strong> $L(P,T(x))$ 表示当真实分布为 $P$ 时，观察到 $X=x$ 时执行决策 $T(X)$ 的损失。</p>
<p>**风险（risk）**表示平均损失，即
$$
R_T(P)=\mathrm E_P\left(L(P,T(X))\right)=\int L(P,T(X))dP.
$$
我们自然希望风险越小越好。对于两个决策规则 $T_1,T_2$，我们称：</p>
<ul>
<li>$T_1$ <strong>as good as</strong> $T_2$，如果 $R_{T_1}(P)\le R_{T_2}(P),\forall P\in\mathcal{P}$,</li>
<li>$T_1$ <strong>better than</strong> $T_2$，如果 $T_1$ <strong>as good as</strong> $T_2$ 且对某个 $P$，$R_{T_1}(P)&lt;R_{T_2}(P)$,</li>
<li>$T_1$ 和 $T_2$ <strong>equivalent</strong>，如果 $R_{T_1}(P)=R_{T_2}(P),\forall P\in\mathcal{P}$.</li>
</ul>
<blockquote>
<p>这个 as good as 应该理解为“不差于”。</p>
</blockquote>
<p>接下来我们有两个问题：</p>
<ul>
<li>如何确定风险函数 $R$</li>
<li>确定了 $R$ 之后，如何根据 $R$ 来选择最优的决策</li>
</ul>
<h3 id="几种最优">几种“最优”</h3>
<p>假设我们有一堆可选的决策规则，记作集合 $\mathfrak{J}$. 给定 $R$ 之后，如何选择“最优”呢？</p>
<p><strong>Optimal</strong>：如果 $T_<em>$ as good as $\mathfrak{J}$ 中其他规则，则称它 <strong>$\mathfrak{J}$-optimal</strong>. 在“所有可能的决策规则”可以构成集合时，若 $T_</em>$ as good as 所有可能的决策规则，则称为 <strong>optimal</strong>。</p>
<blockquote>
<p>Optimal 就是所谓的“完爆”。</p>
</blockquote>
<p>显然，optimal 不一定存在，我们需要新的定义。</p>
<blockquote>
<p>“炉石传说没有完爆！”——银背族长</p>
</blockquote>
<p><strong>Admissible</strong>：如果不存在比 $T_*$ better 的规则，则称它 <strong>$\mathfrak{J}$-admissible</strong>（或 admissible）。</p>
<p><strong>Minimax</strong>：$\sup_P R_{T_<em>}(P)\le\sup_P{R_{T}}(P)$，则 $T_</em>$ 是 minimax。</p>
<p><strong>Bayes-rule</strong>：考虑 Bayes risk
$$
r_T(\Pi)=\int_\mathcal{P}R_T(P)d\Pi(P).
$$
给定 $\Pi$，如果 $T_<em>$ 满足 $r_{T_</em>}\le r_T$，就叫 Bayes rule w.r.t. $\Pi$。要寻找 Bayes rule 下的最优决策，可以考虑 $\mathrm E\left(\mathrm E\left(L(\theta,T(X))|X\right)\right)$.</p>
<h3 id="点估计的评价点估计的风险函数">点估计的评价（点估计的风险函数）</h3>
<h4 id="mse">MSE</h4>
<p>我们把上面的理论套用起来。我们用 $\hat\theta$ 表示估计量，也就是上面的 $T(X)$. 常用的风险函数为<strong>均方误差（mean squared error, MSE）</strong>。MSE 对应的损失函数为 $L(P_\theta,\hat\theta)=(\theta-\hat\theta)^2$，对应的风险函数为
$$
MSE=\mathrm E_\theta\left((\theta-\hat\theta)^2\right).
$$
我们定义<strong>偏差（bias）</strong>：
$$
b_T(\theta)=\mathrm E_\theta(\hat\theta)-\theta,
$$
则有 $
\mathrm E_\theta\left((\theta-\hat\theta)^2\right)=\mathrm E_\theta\left((\theta-\mathrm E\hat\theta)^2\right)+\left(\mathrm E\hat\theta-\theta\right)^2$，即
$$
MSE(\theta)=b^2(\theta)+\mathrm{Var}(\theta).
$$</p>
<h4 id="rao-blackwall-定理">Rao-Blackwall 定理</h4>
<p>如下定理表明，我们可以考虑充分统计量的条件期望来构造更好的估计。</p>
<p><strong>定理（Rao-Blackwall）</strong> 设 $T$ 是充分统计量。若参数空间 $\Theta$ 是凸集，$S_0(X)$ 满足 $\mathrm E_P|S_0|&lt;\infty,\forall P\in\mathcal{P}$. 令 $S_1=\mathrm E(S_0(X)|T)$，则</p>
<ul>
<li>若损失函数 $L(P,a)$ 关于 $a$ 是凸函数，则 $R_{S_1}(P)\le R_{S_0}(P)$；</li>
<li>若 $L(P,a)$ 严格凸，且 $S_0$ 不是 $T$ 的函数，则 $S_1$ better than $S_0$.</li>
</ul>
<p>读者自证不难。（为什么要求 $T$ 是充分统计量？）</p>
<h4 id="umvue">UMVUE</h4>
<p>uniformly minimum variance unbiased estimator. 即对所有 $P$ （一致），$\mathrm{Var}(T_*(X))\le\mathrm{Var}(T(X))$（最小），且无偏（相当于规定 $\mathfrak{J}$ 是无偏估计，然后寻找 MSE 下的最优估计）。我们下面介绍几种寻找 UMVUE 的方法。</p>
<p><strong>方法一：定理（Lehmann-Scheffe）</strong> 设 $T(X)$ 充分且完备，若 $\theta$ 的无偏估计存在，则存在唯一的形如 $h(T)$ 的无偏估计，且 $h(T)$ 是唯一的 UMVUE。</p>
<p>这个定理告诉我们，想获得 UMVUE，可以先找充分且完备的统计量 $T(X)$，然后尝试 $h(T)$，使得 $\mathrm Eh(T)=\theta$. 那如果找不到呢？可以用下面的定理。</p>
<p><strong>方法二：定理</strong> 设 $\mathcal{U}={T(X):\mathrm E(T)=0 \text{ and }\mathrm{Var}(T)&lt;\infty}$，设 $T$ 是参数的无偏估计且 $\mathrm E(T^2)&lt;\infty$. 令内积为 $\langle X,Y\rangle=\mathrm E(XY)$. 则：</p>
<ul>
<li>$T$ 是 UMVUE 的充要条件是 $T\perp \mathcal{U}$（对任意 $P$）.</li>
<li>设 $S$ 是充分统计量，且 $T=h(S)$. 令 $\mathcal{U}_S={U\in\mathcal{U}:U=\varphi(S)}$. $T$ 是 UMVUE 的充要条件是 $T\perp \mathcal{U}_S$ （对任意 $P$）.</li>
</ul>
<p><strong>推论</strong></p>
<ul>
<li>（线性）设 $T_j$ 是 $\eta_j$ 的 UMVUE，方差有限，则 $T=\sum_{j=1}^k c_jT_j$ 是 $\eta=\sum_{j=1}^k c_j\eta_j$ 的 UMVUE.</li>
<li>（唯一性）设 $T_1,T_2$ 是 $\eta$ 的 UMVUE，方差有限，则 $T_1=T_2$ a.s..</li>
</ul>
<p><strong>方法三：C-R 下界</strong>。寻找 UMVUE 的另一种思路是，找到方差的下界。这样只要我们得到一个无偏估计，其方差等于下界，就一定是 UMVUE 了。为此，我们需要一坨子新东西。</p>
<p><strong>定义（Fisher information）</strong> 有一分布族 $\mathcal{P}={f_\theta}$. $X$ 是服从 $f_\theta$ 的样本。如果该存在的都存在，则定义 Fisher information
$$
I(\theta)=\mathrm E\left(\frac{\partial}{\partial\theta}\log f_\theta (X)\right)^2.
$$
<strong>性质1</strong> （自行证明）若 $\dfrac{\partial^2}{\partial\theta^2}f_\theta$ 存在，且满足光滑性条件：
$$
\begin{gather}
\frac{\partial}{\partial\theta}\int f_\theta(x)dx=\int \frac{\partial f_\theta(x)}{\partial\theta}dx, \
\frac{\partial}{\partial\theta}\int \frac{\partial f_\theta (x)}{\partial\theta}dx=\int \frac{\partial^2 f_\theta (x)}{\partial\theta^2}dx,\</p>
<p>\end{gather}
$$
则有
$$
I(\theta)=-\mathrm E\left(\dfrac{\partial^2}{\partial\theta^2}\log f_\theta(X)\right).
$$
<strong>性质2</strong> 若 $X,Y$ 独立，则 $I_{X+Y}=I_X+I_Y$.</p>
<p><strong>定理（Cramer-Rao 下界）</strong> 满足上述光滑性条件时，若 $T(X)$ 是 $g(\theta)$ 的无偏估计，且满足
$$
g'(\theta)=\frac{\partial}{\partial\theta}\int T(x)f_\theta(x)dx=\int T(x)\frac{\partial}{\partial\theta}f_\theta(x)dx,
$$
则 $\mathrm{Var}(T(X))\ge\dfrac{g'(\theta)^2}{I(\theta)}$.</p>
<p>我们可以将其推广到参数为多元的情况。若 $\theta$ 是向量，记 $\dfrac{\partial}{\partial\theta}$ 为梯度（列向量）。则信息矩阵
$$
I(\theta)=\mathrm E\left{\frac{\partial\log f_\theta(x)}{\partial\theta}\left[\frac{\partial\log f_\theta(x)}{\partial\theta}\right]^T\right}.
$$
相应的，光滑性条件也是矩阵（或向量）的每个位置都满足。此时 Cramer-Rao 下界为
$$
\mathrm{Var}(T(X))\ge\left(\frac{\partial g(\theta)}{\partial\theta}\right)^TI^{-1}(\theta)\frac{\partial g(\theta)}{\partial\theta}.
$$
<strong>指数族</strong> 对于指数族，我们又有福利了。若
$$
f_\theta(x)=\exp\left[\eta^T(\theta)T(x)-\xi(\theta)\right]h(x),
$$
则:</p>
<ul>
<li>对满足 $E|S(X)|&lt;\infty$ 的 $S$，上面乱七八糟的光滑性条件都成立，即
<ul>
<li>$
\frac{\partial}{\partial\theta}\int S(x)f_\theta(x)dx=\int S(x)\frac{\partial}{\partial\theta}f_\theta(x)dx
$,</li>
<li>$I(\theta)=-\mathrm E\left(\dfrac{\partial^2}{\partial\theta^2}\log f_\theta(X)\right)$.</li>
</ul>
</li>
<li>此外还有 $
\mathrm{Var}(T)=I(\eta)$,</li>
<li>令 $\psi=\mathrm E(T(X))$，则 $\mathrm{Var}(T)=I^{-1}(\psi)$.</li>
</ul>
<h2 id="假设检验">假设检验</h2>
<p>设 $\mathcal{P}$ 是分布族，$\mathcal{P}_0\in\mathcal{P},\mathcal{P}_1=\mathcal{P}\setminus\mathcal{P}_0$. 一般的假设检验问题需要决定以下两个假设哪个是对的：
$$
H_0:P\in\mathcal{P}_0,\
H_1:P\in\mathcal{P}_1.
$$
动作空间 $\mathbb{A}={0,1}$. 此时的决策规则 $T=I_C(X)$，即 $X\in C$ 时选择 $H_1$，否则选择 $H_0$. $C$ 被称为<strong>拒绝域（rejection region）</strong>（因为拒绝了 $H_0$）。</p>
<h3 id="两类错误">两类错误</h3>
<p>**第一类错误（type I error）**指 $H_0$ 成立，但拒绝了 $H_0$.</p>
<p>**第二类错误（type II error）**指 $H_0$ 不成立，但接受了 $H_0$.</p>
<p>我们定义**功效函数（power function）**为第一类错误的概率，即
$$
\alpha_T(P)=P(X\in C).
$$
假设检验的 <strong>size</strong> 定义为功效的上确界，即
$$
\alpha'=\sup_{P\in\mathcal{P}_0} P(X\in C).
$$</p>
<h2 id="渐近分析slides-19">渐近分析（slides 19）</h2>
<p>许多时候我们无法得到 $T_n$ 的确切分布，这时候考虑 $T_n$ 的极限性质会大有帮助。</p>
<h3 id="一致性">一致性</h3>
<p><strong>定义</strong></p>
<ul>
<li>$T_n(X)$ 是 $\theta$ 的<strong>一致估计（consistent）</strong>，当且仅当 $T_n(X)\overset{p}\to\theta$.</li>
<li>$T_n(X)$ 是 $\theta$ 的<strong>强一致估计（strongly consistent）</strong>，当且仅当 $T_n(X)\overset{a.s.}\to\theta$.</li>
<li>${a_n}$ 是一个正数列，$a_n\to\infty$，称 $T_n(X)$ <strong>$a_n$-consistent</strong>，当且仅当 $a_n[T_n(X)-\theta]=O_P(1)$.</li>
<li>$T_n(X)$ 称为 <strong>$L_r$-consistent</strong>，当且仅当 $T_n(X)\overset{L^r}\to\theta$.</li>
</ul>
<p>显然其他几种一致性都能推出第一种 trivial consistent.</p>
<h3 id="渐近偏差与渐近方差">渐近偏差与渐近方差</h3>
<p><strong>渐近无偏</strong>：$b_n\to 0$.</p>
<p><strong>渐近期望</strong>：数列 $a_n\to\infty$ 或 $a_n\to a&gt;0$，若 $a_n\xi_n\overset{d}\to \xi$ 且 $\mathrm E|\xi|&lt;\infty$，则 $\mathrm E\xi/a_n$ 称为渐近期望。</p>
<p><strong>渐近偏差</strong>：$T_n-\theta$ 的渐近期望。</p>
<p><strong>渐近MSE</strong>：$a_n(T_n-\theta)\overset{d}\to Y$，则 $\text{amse}=\mathrm E Y^2/a_n^2$.</p>
<p><strong>渐近方差</strong>：$a_n(T_n-\theta)\overset{d}\to Y$，则渐近方差为 $\mathrm{Var}Y/a_n^2$.</p>
<p>高维的情况：设 $\hat\theta_n$ 是一列估计（$k$ 维向量），若存在正定矩阵 $V_n(\theta)$ 使得
$$
[V_n(\theta)]^{-1/2}(\hat\theta_n-\theta)\overset{d}\to N_k(0,I_k),
$$
其中 $I_k$ 是单位矩阵，则称 $V_n(\theta)$ 是<strong>渐近协方差矩阵</strong>。</p>
<p>若 Fisher 信息矩阵正定，且渐近方差满足 $V_n(\theta)=I_n(\theta)^{-1}$ （“CRLB”），则称之为 <strong>asymptotially efficient</strong> 或 <strong>aymptotically optimal</strong>.</p>
<p>两组估计，渐近协方差分别为 $V_{1n}(\theta)$ 和 $V_{2n}(\theta)$，若对足够大的 $n$，有 $\forall \theta\in\Theta,\quad V_{2n}(\theta)-V_{1n}(\theta)$ 半正定；且存在某个 $\theta$ 使其正定，则称 $\hat\theta_{1n}$ <strong>asymptotically more efficient than</strong> $\hat\theta_{2n}$.</p>
<blockquote>
<p>注意：对于渐近无偏的估计，CRLB 不一定对渐近方差成立，例：Hodges' estimator, Lec23 p13.</p>
</blockquote>
<p><strong>Asymptotic relative efficiency</strong>: $T'<em>n$ 相对于 $T_n$ 的渐近效率为
$$
e</em>{T'<em>n,T_n}(P)=\frac{\text{amse}</em>{T_n}(P)}{\text{amse}_{T'<em>n}(P)}.
$$
如果 $\limsup_n e</em>{T'_n,T_n}(P)\le 1$，且存在 $P$ 使 $&lt;$ 成立，则称 $T_n$ <strong>asympotically more efficient</strong>.</p>
<p><strong>定理（$\delta$-method）</strong> 设 $U_n$ 满足 $a_n(U_n-\theta)\overset{d}\to Y$ 且 $EY^2&lt;\infty$，$a_n&gt;0$，$a_n\to\infty$. 设 $g$ 在 $\theta$ 处可微，$T_n=g(U_n)$ 是$\vartheta=g(\theta)$ 的估计量，则 $\text{amse}_{T_n}=E{[g'(\theta)Y]^2}/a_n^2$, $T_n$ 的渐近方差为 $[g'(\theta)^2\mathrm{Var}Y]/a_n^2$.</p>
<h3 id="点估计的渐近性质">点估计的渐近性质</h3>
<h4 id="矩估计-1">矩估计</h4>
<p>对于矩估计，若 $h^{-1}$ 存在，由大数定律知它是 <strong>strongly consistent</strong>.</p>
<p>若还有 $h^{-1}$ 可微且 $E|X_1|^{2k}&lt;\infty$ （$k$ 为参数个数），由 CLT 知矩估计 $\sqrt{n}$-consistent.</p>
<p>若 $k=1$，则 $\text{amse}_{\hat\theta_n}(\theta)=g'(\mu_1)^2\sigma^2/n$.</p>
<h4 id="umvue-1">UMVUE</h4>
<p>UMVUE 都是一致无偏的。</p>
<h4 id="样本分位数">样本分位数</h4>
<p>我们经常用样本分位数做估计量，因此有必要研究它。对 $\gamma\in(0,1)$，第 $\lfloor \gamma n\rfloor$ 个次序统计量被称为 <strong>$\gamma$-sample quantile</strong>. 有如下结论：</p>
<p><strong>定理</strong> 设 $X_i$ i.i.d.，cdf 为 $F$，若 $F(\theta)=\gamma$，$F'(\theta)$ 存在且不为 0，则第 $\lfloor \gamma n\rfloor$ 个次序统计量 $\tilde{\theta}_n$ 满足
$$
\sqrt{n}\left(\tilde{\theta}_n-\theta\right)\overset{d}\to N\left(0,\frac{\gamma(1-\gamma)}{F'(\theta)^2}\right).
$$</p>
<blockquote>
<p>证明用到 Berry–Esseen Theorem，略</p>
</blockquote>
<h4 id="mle">MLE</h4>
<p><strong>定理</strong> 设 $\theta_0$ 为实际参数，并满足以下条件：</p>
<ul>
<li>
<p>$\Theta$ 是紧集，</p>
</li>
<li>
<p>对任意 $x$，$f(x|\theta)$ 关于 $\theta$ 连续，</p>
</li>
<li>
<p>存在控制函数 $M(x)$ 使得 $E_{\theta_0}|M(X)|&lt;\infty$ 且
$$
\left|\log f(x|\theta)-\log f(x|\theta_0)\right|\le M(x),\quad\forall x,\theta,
$$</p>
</li>
<li>
<p>（一致性）$f(x|\theta)=f(x|\theta_0)$ 则 $\theta=\theta_0$.</p>
</li>
</ul>
<p>此时，MLE $\hat\theta_n\overset{a.s.}\to\theta_0$.</p>
<blockquote>
<p>注：</p>
<ol>
<li>连续性可以替换成上半连续，即对任意 $x$，有</li>
</ol>
<p>$$
\lim_{\rho\to 0}\sup_{|\theta'-\theta|&lt;\rho }f(x|\theta') =f(x|\theta).
$$</p>
<ol start="2">
<li>可以推广到任意度量空间，只需把 $|\theta-\theta_0|$ 换成度量 $d(\theta,\theta_0)$.</li>
<li>控制函数 $M(x)$ 的存在性是关键。</li>
</ol>
</blockquote>
<h4 id="m-估计-1">M-估计</h4>
<p>类似于 MLE，有如下结论：</p>
<p><strong>定理</strong> 有 $S_n,S$ 满足</p>
<ol>
<li>$
\sup_{\theta\in\Theta}|S_n(\theta)-S(\theta)|\overset{p}\to 0$,（一致收敛）</li>
<li>$\sup_{\theta:d(\theta,\theta_0)\ge\rho}S(\theta)&lt;S(\theta_0)$，（well-separation）</li>
</ol>
<p>若估计量 $\hat\theta_n$ 满足 $S_n(\hat\theta_n)\ge S_n(\theta_0)-o_P(1)$，则 $d(\hat\theta_n,\theta_0)\overset{p}\to 0$.</p>
<h4 id="rle">RLE</h4>
<p>RLE 指的是 roots of likelihood equation，使得 $\dfrac{\partial}{\partial\theta}\log L_n(\theta)=0$ 的点。它和 MLE 有着千丝万缕的联系。事实上，在一定的正则性条件下，它收敛到真实参数，这使得 RLE 具有一致性。</p>
<p><strong>正则性条件</strong>（basic regularity conditions） 设 $\theta_*$ 是真实值，则</p>
<ol>
<li>
<p>$\Theta$ 是 $\mathbb{R}^k$ 中的开集，</p>
</li>
<li>
<p>$f(x|\theta)$ 二阶连续可微，且一二阶导数均可和积分交换，</p>
</li>
<li>
<p>（控制函数）设 $\Psi(x,\theta)=\dfrac{\partial^2}{\partial\theta\partial\theta^T}\log f(x|\theta)$（是矩阵），则存在常数 $c$ 和非负函数 $H$ 使得 $EH(X)&lt;\infty$ 且
$$
\sup_{|\theta-\theta_*|&lt;c}|\Psi(x,\theta)|\le H(x).
$$</p>
</li>
<li>
<p>（identifiability）$f(x|\theta)=f(x|\theta_<em>)$ 则 $\theta=\theta_</em>$.</p>
</li>
</ol>
<p><strong>定理</strong>（RLE的一致性） 在上述正则性条件下，存在一列 $\hat\theta_n$ 使得 $\dfrac{\partial}{\partial\theta}\log L_n(\hat\theta_n)=0$ 且 $\hat\theta_n\overset{a.s.}\to\theta_*$.</p>
<p>此外，我们可以讨论 RLE 的渐近正态性。</p>
<p><strong>定理</strong> 设正则性条件成立，且 Fisher 信息矩阵在 $\theta_<em>$ 处正定，则对任意的一致 RLE 序列 $\tilde{\theta}<em>n$（比如上一个定理中收敛的 RLE 序列），有
$$
\sqrt{n}(\tilde\theta_n-\theta</em></em>)\overset{d}\to N(0,I(\theta_*)^{-1}).
$$</p>
<blockquote>
<p>如果 MLE 是一致的，且 MLE 就是 RLE，则它可以用来说明 MLE 的渐近正态性。</p>
</blockquote>
    </div>
</article>



<div class="post-comment" data-comment="utterances">
    
    
    <script src="https://utteranc.es/client.js"
        repo="jcq15/blog"
        issue-term="title"
        theme="github-light"
        crossorigin="anonymous"
        async>
    </script>
</div>


            </div>
            <aside class="col-12 col-md-3 float-left sidebar">
    
    <div class="sidebar-item sidebar-pages">
        <h3>页面</h3>
        <ul>
            
            <li>
                <a href="/">主页</a>
            </li>
            
            <li>
                <a href="/archives/">文章</a>
            </li>
            
            <li>
                <a href="/about/">关于</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>链接</h3>
        <ul>
            
            <li>
                <a href="https://github.com/jcq15" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>标签</h3>
        <div>
            
            <span>
                <a href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%8A%80%E6%9C%AF/">技术</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B2%99%E9%9B%95/">沙雕</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B8%B8%E6%88%8F/">游戏</a>
            </span>
            
            <span>
                <a href="/tags/%E7%A7%91%E6%99%AE/">科普</a>
            </span>
            
            <span>
                <a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/">经验分享</a>
            </span>
            
        </div>
    </div>
    <div class="sidebar-item sidebar-toc">
        <h3>目录</h3><nav id="TableOfContents">
  <ul>
    <li><a href="#点估计">点估计</a>
      <ul>
        <li><a href="#矩估计">矩估计</a></li>
        <li><a href="#极大似然估计">极大似然估计</a></li>
        <li><a href="#m-估计">M-估计</a></li>
      </ul>
    </li>
    <li><a href="#估计的评价">估计的评价</a>
      <ul>
        <li><a href="#一般理论">一般理论</a></li>
        <li><a href="#几种最优">几种“最优”</a></li>
        <li><a href="#点估计的评价点估计的风险函数">点估计的评价（点估计的风险函数）</a></li>
      </ul>
    </li>
    <li><a href="#假设检验">假设检验</a>
      <ul>
        <li><a href="#两类错误">两类错误</a></li>
      </ul>
    </li>
    <li><a href="#渐近分析slides-19">渐近分析（slides 19）</a>
      <ul>
        <li><a href="#一致性">一致性</a></li>
        <li><a href="#渐近偏差与渐近方差">渐近偏差与渐近方差</a></li>
        <li><a href="#点估计的渐近性质">点估计的渐近性质</a></li>
      </ul>
    </li>
  </ul>
</nav></div>
</aside>
        </div>
        <div class="btn">
    <div class="btn-menu" id="btn-menu">
        <i class="iconfont icon-grid-sharp"></i>
    </div>
    <div class="btn-toggle-mode">
        <i class="iconfont icon-contrast-sharp"></i>
    </div>
    <div class="btn-scroll-top">
        <i class="iconfont icon-chevron-up-circle-sharp"></i>
    </div>
</div>
<aside class="sidebar-mobile" style="display: none;">
  <div class="sidebar-wrapper">
    
    <div class="sidebar-item sidebar-pages">
        <h3>页面</h3>
        <ul>
            
            <li>
                <a href="/">主页</a>
            </li>
            
            <li>
                <a href="/archives/">文章</a>
            </li>
            
            <li>
                <a href="/about/">关于</a>
            </li>
            
            <li>
                <a href="/index.xml">RSS</a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-links">
        <h3>链接</h3>
        <ul>
            
            <li>
                <a href="https://github.com/jcq15" target="_blank"><span>GitHub</span></a>
            </li>
            
        </ul>
    </div>
    
    <div class="sidebar-item sidebar-tags">
        <h3>标签</h3>
        <div>
            
            <span>
                <a href="/tags/%E5%AD%A6%E4%B9%A0/">学习</a>
            </span>
            
            <span>
                <a href="/tags/%E6%8A%80%E6%9C%AF/">技术</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B2%99%E9%9B%95/">沙雕</a>
            </span>
            
            <span>
                <a href="/tags/%E6%B8%B8%E6%88%8F/">游戏</a>
            </span>
            
            <span>
                <a href="/tags/%E7%A7%91%E6%99%AE/">科普</a>
            </span>
            
            <span>
                <a href="/tags/%E7%BB%8F%E9%AA%8C%E5%88%86%E4%BA%AB/">经验分享</a>
            </span>
            
        </div>
    </div>
    
    
    
    <div class="sidebar-item sidebar-toc">
        <h3>目录</h3>
        <nav id="TableOfContents">
  <ul>
    <li><a href="#点估计">点估计</a>
      <ul>
        <li><a href="#矩估计">矩估计</a></li>
        <li><a href="#极大似然估计">极大似然估计</a></li>
        <li><a href="#m-估计">M-估计</a></li>
      </ul>
    </li>
    <li><a href="#估计的评价">估计的评价</a>
      <ul>
        <li><a href="#一般理论">一般理论</a></li>
        <li><a href="#几种最优">几种“最优”</a></li>
        <li><a href="#点估计的评价点估计的风险函数">点估计的评价（点估计的风险函数）</a></li>
      </ul>
    </li>
    <li><a href="#假设检验">假设检验</a>
      <ul>
        <li><a href="#两类错误">两类错误</a></li>
      </ul>
    </li>
    <li><a href="#渐近分析slides-19">渐近分析（slides 19）</a>
      <ul>
        <li><a href="#一致性">一致性</a></li>
        <li><a href="#渐近偏差与渐近方差">渐近偏差与渐近方差</a></li>
        <li><a href="#点估计的渐近性质">点估计的渐近性质</a></li>
      </ul>
    </li>
  </ul>
</nav>
    </div>
    
    
  </div>
</aside>
    </main>

    <footer>
    <div class="container-lg clearfix">
        <div class="col-12 footer">
            
            <p>
                除特殊注明部分，本站内容采用 <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 进行许可。
            </p>
            
            <span>&copy; 2021-2021
                <a href="https://blog.shadiao.online">精神病人</a>
                 | <a href="https://github.com/jcq15/blog">Source code</a> 
                | 基于 <a href="https://github.com/dsrkafuu/hugo-theme-fuji/"
                   target="_blank">Fuji-v2</a> &amp; <a href="https://gohugo.io/"
                                                    target="_blank">Hugo</a> 构建
            </span>
        </div>
    </div>
</footer>

    
<script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/lazysizes@5.3.0/lazysizes.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/components/prism-core.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/prismjs@1.23.0/plugins/autoloader/prism-autoloader.min.js"></script>



<script defer src="/assets/js/fuji.min.js"></script>


<script>
  MathJax = {
    tex: {
      inlineMath: [["$", "$"]],
    },
    displayMath: [
      ["$$", "$$"],
      ["\[\[", "\]\]"],
    ],
    svg: {
      fontCache: "global",
    },
  };
</script>
<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script
  id="MathJax-script"
  async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
></script>



</body>

</html>